\documentclass[12pt, a4paper, oneside]{article}
\usepackage{amsmath, amssymb, amsfonts, amsthm, commutative-diagrams, geometry, graphicx, mathrsfs, indentfirst, pgfplots, url, verbatim}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage[runin]{abstract}
%\usepackage{titlesec}
\pgfplotsset{compat=1.5}
\renewcommand{\refname}{\quad}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{plain}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{corollary}[definition]{Corollary}
\theoremstyle{remark}
\newtheorem{remark}[definition]{Remark}

\title{Prospects for Further Unification in the Theory of Elementary Particles}
\author{Steven Weinberg}
\date{1 July 1982}

\begin{document}
\maketitle
%\renewcommand{\abstractname}{}
%\begin{abstract}\\
%\textbf{Abstract}\\
%\textbf{Keywords:}
%\end{abstract}
\section*{Speech}
\subsection*{Introduction}
Physicists naturally try to see phenomena in simple terms. You might say that the primary justification for doing elementary particle physics, with all its expense and difficulty, is the opportunity it gives us of seeing all of nature in somewhat simpler terms. Great progress had been made a few years ago, say from the late 1960s to the mid-1970s, in clarifying the nature of the elementary particles and their interactions.\par
Then, starting about the mid-1970s, we began to confront a series of problems of much greater difficulty, and I would have to say that very little progress has been made. I would like, first of all today, to remind you of what the present understanding of elementary particle physics is as it was already formulated by the mid-1970s, and then, for the larger part of my talk, discuss the attempts that have been made since the mid-1970s to go beyond this to the next level of simplicity. The present understanding of elementary particle physics, I would say, is based on three chief elements. First of all, there is a picture of the interactions of the elementary particles as being all extremely similar to the one interaction that was earlier well understood, the electromagnetic interaction. You know that the electromagnetic interaction is carried by a massless particle of spin one, the photon, which, for example, is exchanged between the electron and the proton in the hydrogen atom.\par
In the present understanding of elementary particle physics, there are twelve “photons” (, and I should do this, meaning that the word is put in quotation marks). There are twelve “photons” which carry all the forces that we know of between the elementary particles. These twelve “photons” comprise, first of all, the familiar old photon, which is emitted, say, by an electron or by any charge particle; and then by three siblings, three family members called intermediate vector bosons: a $W^-$, a $W^+$ and a $Z^0$, which are emitted, for example, when leptons change their charge when electrons turn into neutrinos or neutrinos turn into electrons and the neutral one, the $Z^0$, is emitted by electrons and neutrinos when they don't change their charge.\par
Similarly, the $W$ and the $Z$ are also emitted by quarks when they do or do not change their charge, respectively. In addition to the four “photons” of the electroweak interactions, there are eight similar particles known as gluons that Sam Ting has already mentioned, which are emitted when quarks change not their charge, but a different property which has been humorously named their color so that a green quark may turn into a red quark emitting a red green gluon. There are 3 colors, and hence there are 8 gluons. You may ask why not 9 gluons? And I will tell you if you ask me privately later.\par
\subsection*{A}
Now, in electromagnetism, we have not only the general idea of a “photon”, but a very clear picture of a symmetry principle of nature which determines the properties of the “photon” and determines, in particular, all of its interactions, the principle known as gauge invariance. In a sense, from the point of view of the theoretical physicists, the photon is the way it is because gage invariance requires it to be that way. The twelve “photons” of the elementary particle interactions as we know them are also governed by a principle of gauge invariance. But the group of gauge transformations is larger and it is known mathematically as $SU(3)\times SU(2)\times U(1)$. The $SU(2)\times U(1)$ is a four-dimensional group which governs the four particles of the electroweak interactions. The $W$ and the $Z$ transmit the weak nuclear force which gives rise to radioactive $\beta$ decay.\par
So this whole set of interactions are called the electroweak interactions, and the $SU(3)$ governs the eight gluons which give rise to the strong nuclear forces. This is sometimes called “the 3-2-1 theory”. The theory of the eight gluons by itself is known as quantum chromodynamics. The electric charge of the electrons, say, is in fact just a peculiar weighted average of coupling constants $g$ and $g'$ associated with these two groups, $SU(2)$ and $U(1)$. $g$ and $g'$ play the same role for these groups of gauge transformations that the electric charge played in the older theory of quantum electrodynamics and the electric charge, in fact, is given by a simple formula in terms of them. And similarly, there is another coupling constant. A coupling constant is just a number that tells you the strength with which these particles are emitted and absorbed. There's another coupling constant that tells us how strongly gluons are emitted (, say, when quarks change their color,) known as $g_s$ for the group $SU(3)$. Now, this is a very pretty picture, especially since, based as it is on familiar old ideas of gauge invariants, it requires us really to learn very little new, which is always to be preferred.\par
But there is an obvious difficulty with it. That is the gauge invariance requires that the vector particles, the spin-1 particles that transmit the force, have zero mass. Just as, for example, electromagnetic gauge invariance requires the photon to have zero mass. But of course, the $W$ and the $Z$ do not have zero mass. They have masses which are so large that no one so far has been able to produce them, although we have strong reasons to think we know where they are.\par
\subsection*{B}
The explanation for this is now universally believed to be that the gauge symmetry, although precise and exact in no sense approximate, is subject to a phenomenon known as spontaneous symmetry breaking. That is, these are symmetries of the underlying equations but they are not realized in the physical phenomena. This is a lesson that elementary particle physicists learned from solid state physicists who understood it much earlier than we did. That symmetries can be present at the deepest level and yet not apparent in the phenomena. The symmetries are just as fundamental as if they were not broken, but they're much harder to see. Because the electroweak symmetry of $SU(2)\times U(1)$ is spontaneously broken, the $W$ and the $Z$ have masses. The $m_W$ is greater than 40 GeV, the $m_Z$ is greater than 80 GeV, and the precise values are determined by an angle which just basically tells you the ratio of these two coupling constants. The angle is measured in a great variety of experiments and on the basis of that we believe the $W$ will be at a mass of about 80 GeV and the $Z$ will be at a mass of about 90 GeV. Because we anxiously await confirmation of that. The second of the three ingredients or the three elements on which our present understanding is based is the menu of elementary particles. I won't dwell on this.\par
There are six different varieties of quarks of which five have been discovered and the 6th is anxiously awaited. Each one of these varieties, sometimes called flavors of quarks according to quantum chromodynamics, comes in three colors so that altogether there are 18 quarks. And then in parallel to the quarks there are doublets of leptons (with) neutrino electron, and then the muon behaving like a heavier electron has its own associated neutrino, and the $\tau$-lepton has its associated neutrino. Physical processes are most simply seen in terms of these quarks and leptons. So, for example, when a neutron decays a state which was originally an up quark and two down quarks of three different colors turns into two up quarks and a down quark of three different colors, a $W^-$ being emitted, which then turns into an electron and an antineutrino. This menu of elementary particles is in no sense forced on us by theory except for the structure of doublets of quarks and leptons and color triplets of quarks. The fact that there are six flavors is just taken from experiment and it has to be regarded as just an independent empirical foundation of our present understanding.\par
\subsection*{C}
The third of the foundations of the present understanding of physics is more mathematical but I think equally important the idea of renormalizability. Renormalizability is very simply the requirement that the physical laws must have the property that whenever you calculate a physically relevant quantity you don't get nonsense, you don't get a divergent integral. You get an integral which converges and that is a finite number. I think we'll all agree that that's a desirable quality of a physical theory. The physical theories that satisfy that requirement are actually very easy to distinguish.\par
If an interaction of some sort has a coupling constant $g$ like the coupling constants $g$, $g'$ and $g_s$ that I discussed earlier and if that coupling constant has the dimensions of mass to some power $-d$, let's say, a negative power, $d$ is positive. And when I talk about dimensions, I will always be adopting the physicist system of units in which Planck's constant and the speed of light are 1($\hbar = c = 1$). Then because the coupling constant has the dimensions of a negative power of mass. The more powers of coupling constant you have in the matrix element for any physical process, the more powers of momentum which have the dimensions of a positive power of mass (mass to the 1st power), the more powers of momentum you will have to have in the integrals, so that as you go to higher and higher order in the coupling constant, you get more and more powers of momentum in the integrals. And the integrals will therefore diverge worse and worse.\par
That's a bad thing. That's not a renormalizable theory. The allowed interactions, the renormalizable theories, are those with coupling constants therefore which are not negative powers of mass but which are either dimensionless like the electric charge of the electron or a positive power of mass like, for example, any mass. A physically satisfactory theory ought to be one which contains only such coupling constants. Now that is enormously predictive, because the energy densities or the Lagrangians that determine physical laws always have a fixed dimensionality of mass to the 4th power. And I remind you that I'm using units in which Planck's constant and the speed of light are 1, so energy has the unit of mass and length has the unit of inverse mass. So therefore, for coupling constant it appears in an energy density and it multiplies an operator, a function $f$ with the dimensionality mass to some power $d$, then the dimensionality of the coupling constant must be just the dimensionality of the Lagrangian, $4-d$. So therefore, in order to keep the dimensionality of the coupling constant positive or zero, we must have the dimensionality of the operators four or less. But almost everything has positive dimensionality: fields have dimensionality 1 for boson fields or 3/2 for spinor fields; derivatives, spacetime derivatives, have dimensionality 1 and therefore as you make an interaction more and more complicated its dimensionality inevitably increases, but the allowed interactions have dimensionality only 4 or less. And therefore, the principle of renormalizability limits the complexity of interactions that are allowed in physics. This is just what physicists need. They need something to tell them: “Do not think about all conceivable theories. Think about a limited set of simple theories.” The limited set of simple theories that we allow ourselves to think about are those with interactions whose dimensionalities are 4 or less and therefore are sharply limited in the number of fields and derivatives that they can have.\par
\subsection*{Symmetry Principles}
In fact, so powerful of these limitations that principles A, B and C determine a physical theory uniquely except for a few free parameters. The free parameters are things like the electric charge of the electron, the fermi coupling constant of $\beta$ decay, the mixing angle between the $Z$ and the photon, a scale parameter of quantum chromodynamics which tells us where the strong coupling constant begins to become very large, and of course, all the quark and lepton masses and masses for other particles called Higgs bosons that I haven't mentioned. But aside from this fairly limited set of free parameters, not as limited as we would like but still not enormous number of free parameters, the physical theory of elementary particles in their observed interactions is completely determined; and not only determined, but agrees, as far as we know, with experiment.\par
One of the features of this understanding (, which I think is perhaps not as widely emphasized as I would like, to me it seems one of the most satisfactory aspects of what we know about physics,) is that the conservation laws of physics that were painfully deduced from experiment in the 1930s, 1940s and 1950s and 1960s are now understood as often approximate consequences of these deeper principles. The theory, as constrained by gauge invariance and by renormalizability and other fundamental principles, cannot be complicated enough to violate these symmetry principles. So, for example, as long as you assume that certain quark masses are small, the strong interactions must obey the symmetries of isotopic spin invariance chirality in the Eightfold Way of Gell-Mann and Ne'eman \cite{Gell-Mann:1961omu,ne1961derivation} which were previously deduced on the basis of data as whatever the values of the quark masses, the strong and the electromagnetic interactions must conserve the quantities known as “Strangeness” and “charge conjugation(Q)” invariance, and with certain qualifications, “Parity” and “time reversal” invariance. And independently of the values of the quark's masses and without any qualifications at all, the strong, weak and electromagnetic interactions must conserve “Baryon (number)” and “Lepton number”. There is no way of writing down a theory complicated enough to violate these conservation laws, a theory that would be consistent with the principles that I've described.\par
(Host: “Much time to have.” Weinberg: “Not bad~”)\par
This understanding of the physical origin of the symmetry principles leads us to a further reflection. We now understand why, let us say, Strangeness is conserved. Strangeness, the property that distinguishes a $K$ meson from a $\pi$ meson, or a hyperon from a nucleon, is not conserved because the laws of nature contain, on some fundamental level, a principle of Strangeness. Strangeness is concerned as a more or less accidental consequence of the theory of strong interactions known as quantum chromodynamics. The theory simply can't be complicated enough to violate the principle of Strangeness conservation. Because Strangeness conservation can be understood without invoking Strangeness as a fundamental physical conservation law, we are led to reflect that perhaps it is not a fundamental symmetry, and perhaps when you widen your scope beyond the strong interactions, you will see that Strangeness is not conserved. That's, in fact, of course true, and it's been known to be true since the first days that people started talking about Strangeness conservation. The weak interactions don't conserve Strangeness.\par
For example, a hyperon is able to decay into an ordinary proton or neutron, violating the conservation of the Strangeness quantum number that distinguishes the two. In exactly the same way, because we now understand that Baryon and Lepton number,... (By the way, Baryon number is just a number, which counts the number of quarks, it's 1/3 for each quark; and Lepton number is a number which counts the number of leptons. It's 1 for each lepton. The conservation of Baryon and Lepton number, for example, prohibits processes like the proton decaying into a positron and a $\pi^0$, which would otherwise be allowed.) ...because the conservation of Baryon and Lepton number is understood as a dynamical consequence of the present theory of electroweak and strong interactions and the principle of renormalizability, there is no reason to doubt that when we go to a wider context, this conservation law will be found to be violated. Because it is not needed as a fundamental conservation law. It is understood without it being needed on a fundamental level.\par
A great deal of attention has been given to this possibility that Baryon and Lepton number are not conserved. Suppose, for example, that there are exotic particles with masses much greater than the $W$ or the $Z$. Let me take the mass $M$, it's just some characteristic mass scale for a new world of exotic particles that have not yet been discovered. And by “exotic”, I mean rather precisely particles with different quantum numbers for the gauge symmetries, $SU(3)\times SU(2)\times U(1)$, than the known quarks and leptons and gauge bosons. The world that we know of is just the world of those particles that have much smaller masses than this new scale $M$. And that world is described, since we're not looking at all of physics (and) we're only looking at part of physics, not by a fundamental field theory but (by) what's called an effective field theory.\par
\subsection*{Effective Field Theory}
We should describe our present physics in terms of an effective Lagrangian. That effective Lagrangian, since it's not the Ultimate Theory of Physics, might be expected to contain nonrenormalizable as well as renormalizable terms in the same way that when Euler and Heisenberg \cite{heisenberg1936folgerungen} in the mid-1930s wrote down an effective Lagrangian for the scattering of light by light at energies much lower than the mass of the electron, they wrote down a nonrenormalizable theory because they weren't working on a fundamental level, but only with an effective theory that was valid as a low energy approximation. The effective theory should contain nonrenormalizable terms and as I indicated before, these are terms whose coupling constant has the dimensionalities of a negative power of mass. That is, we have operators $\mathcal{O}$ with dimensionality $d$ and coupling constants with dimensionality $1/[mass]^{d-4}$. And what mass would this be? Well, it would have to be the mass of the fundamental scale of the particles that have been eliminated from the theory the same way the electron is eliminated from electrodynamics and the Euler-Heisenberg theory of the scattering of light by light. This tells us, then, that the reason that physics appears to us to be dominated by renormalizable interactions at low energies is not because the nonrenormalizable interactions aren't there, but because they're greatly suppressed by negative powers of some enormous mass.\par
And we should expect in the physics of low energies to find not only the renormalizable interactions of the known electroweak and strong interactions, but much weaker subtler effects due to nonrenormalizable interactions suppressed by very large masses in the denominator of the coupling constant. There has been work by myself \cite{Weinberg:1979sa} and Wilczek and Zee \cite{Wilczek:1979hc} to catalog all possible interactions of this sort up to dimension 6 or 7 operators. The lowest dimension of operators that can produce Baryon violation, turns out, are dimension 6 operators and hence, according to the general rules I've given you, are suppressed by 2 powers of a super large mass. A catalog has been made of these dimension-6 operators have the form quark, quark, quark, lepton.\par
A catalog has been made of all of these interactions. And it turns out that they all satisfy the principle that although they violate baryon and lepton conservation, they violate them equally. So that, for example, the proton can decay into an antilepton. The neutron can also decay into an antilepton. Neutron can decay into $e^++\pi^-$, but the neutron cannot decay into a lepton, neutron cannot decay into $e^-+\pi^+$. And there are other consequences of the simple structure of these operators, something like a $\delta I$-half rule: the decay rate of the proton into a positron is one half the decay rate of the neutron into a positron. We can say all these things with great confidence without being at all sure that protons and neutrons actually decay. The decay rate of the proton and the neutron that is decay at an observable rate. The decay rate of the proton, let us say, will be suppressed in the matrix element by 2 powers of a super large mass. And there's sure to be a coupling constant factor like the Fine Structure Constant. You square the matrix element and multiply it by a phase space factor, the proton mass to the 5th power, to get a decay rate. From this, the big unknown in this formula for the proton decay rate is, of course, the super heavy mass scale. We know the proton is more stable than… well its decay, its lifetime, is longer than $10^{30}$ years, and therefore, this mass must be very large indeed. It must be larger than about $10^{14}$ GeV. There are other effects that violate known symmetries. Lepton number is violated by an operator that has dimensionality not 6, but only 5. And this produces neutrino masses of the order of 300 $GeV^{1/2}$ divided by the super heavy mass scale.\par
That's a very low neutrino mass. That's less than 1V if the mass scale is greater than $10^{14}$ GeV. Now, there is other debris which might be expected to be found in the low energy world and I simply won't have time to discuss this. In a sense, gravity itself can be regarded as the debris in our low energy effective field theory of a more fundamental theory that describes physics at a mass scale above $10^{14}$ GeV. But why in the world should there be a mass scale so much larger, 12 orders of magnitude larger than the highest masses that we're used to considering?\par
\subsection*{Grand Unification}
A possible answer comes from the general idea of Grand Unification. Grand Unification is very simply the idea that the strong and electroweak gauge groups are all parts of a larger gauge group which is here simply denoted $G$. Just as the electroweak gauge group is spontaneously broken giving masses to the $W$ into the electromagnetic gauge group. And that's why the $W$ and the $Z$ are so heavy and have not yet been discovered. The grand gauge group $G$ is assumed to be broken at a very high energy scale $M$ into its ingredients $SU(3)\times SU(2)\times U(1)$, and one coupling constant will turn out hopefully to generate the strong and the electroweak coupling constants in the same way that the two electro weak coupling constants combine together to give the electric charge.\par
Another hope here, equally important, is that the quarks and leptons would be unified into a single family so that we wouldn't have red, white, blue and lepton colors, but we would have one quartet for each flavor of quarks and leptons. Models which realize some of these ideas were proposed beginning in 1973, starting with the work of Pati and Salam 
 \cite{pati1974lepton} and then Georgi and Glashow \cite{georgi1974unity} and then Fritzsch and Minkowski \cite{Fritzsch:1974nn} and then many other people. But an obvious difficulty with any model of this sort is the fact that the strong coupling constant, as its name implies, is much stronger than the electroweak couplings. Sam Ting told us that the fine structure constant for the strong interactions is about 0.2, and we all know the fine structure constant for the electromagnetic interactions is $1/137$. How can two such different strengths of force arise from the same underlying coupling constant $g_G$?\par
The answer, which is now I think the most popular, was proposed in 1974 by Georgi, Quinn and myself \cite{georgi1974hierarchy}. Our answer was that these coupling constants are indeed related to a fundamental coupling constant. But they're related only at a super large mass scale $M$. The strong and electroweak couplings which are indicated here as these three curves are not really constants. They're functions of the energy at which they are measured. This is well known in quantum electrodynamics, the property of asymptotic freedom, means that the coupling constant shrinks with energy.\par
The coupling constants of the electroweak force: one of them shrinks, one of them increases. One imagines there might be a point at which they all come together at some very high energy. Indeed, there is such a point. But since this variation with energy is very slow, it's only logarithmic, the energy at which the coupling constants come together is exponentially large. It's given by the formula that
$$\log\frac{M}{m_W}=\frac{4\pi^2}{11E^2}$$
, where $E$ is the electric charge, with a correction due to the strong interactions. And that comes out to be about $4\sim 8 \times 10^{14}$ GeV. So, we see now why there has to be a scale of energies greater than $10^{14}$ GeV, it's to give the strong interactions time to get as weak as the electroweak interactions. These three curves are coming together at one point. It's not easy to get three curves to intersect at a single point. And in fact, the way it's done is by carefully adjusting the data at the low energy end to make them aimed in the right direction so that they'll all hit at the same point. That careful adjustment of the data, I can put in a slightly more complimentary way of saying that we predict, and this was done by Georgi, Quinn and me, we predict certain ratios of these coupling constants which can be expressed as a prediction of the value of the mixing angle between the $Z$ and the photon.\par
That prediction was in conflict with experiment in 1974, in agreement with experiment now. And it's the experiment that changed, not the theory. There are a great many problems. I would say, in fact, that this prediction of this mixing angle is the only tangible success, quantitative success so far of Grand Unification. There are a number of problems with further progress.\par
One problem is that we have had no convincing explanation of the pattern of quark and lepton masses. By convincing explanation, I mean more than a theory in which you have enough free parameters to arrange things the way you want, but something that really gives you a feeling you understand it. There's been no convincing explanation of the pattern of generations that we have not only an up-down electron generation and a charm-strange muon generation, but a third generation, maybe a fourth generation. We don't know why any of that is true. Perhaps the most puzzling problem of all: we have no fundamental explanation of the hierarchy of forces, that is, that there is a ratio of 12 orders of magnitude between the symmetry breaking scale of the grand gauge group and the electroweak gauge group. We know that that's true. We think we know it's true because the strong forces at low energy are so much stronger than the electroweak forces. But where this number of $10^{12}$ comes from is endlessly speculated about. There are many, many ideas, but there's no one idea that really convinces people.\par
And finally, there's no one model that stands out as the obvious model. There are many candidate models of Grand Unification, but since all of them leave A, B and C ununderstood we can't really attach our allegiance to any one of these models. There is a further development which started in 1974 which has occupied a large part of the attention of theoretical physicists in the succeeding years. This is a symmetry called supersymmetry, invented although there were precursors, invented by Wess and Zumino \cite{wess1974supergauge,wess1974lagrangian} and then carried on by Salam and Strathdee \cite{salam1974super} and many other people.\par
\subsection*{Supersymmetry}
Supersymmetry is a symmetry which operates in a sense orthogonally to the symmetries that I've been discussing up till now. The electroweak gauge symmetry, for example, connects the neutrino and the electron, both particles of the same spin but different charge. Supersymmetry connects particles of different spin but the same charge, flavor, color, etc. For example, supersymmetry would connect the electron which has spin-1/2 with another particle that might have spin-0 or spin-1. It had been thought that such symmetries were impossible and in fact they're almost impossible. There is a theorem by Haag, Łopuszański, and Sohnius \cite{haag1975all}, terribly important theorem, that tells us that the kind of supersymmetry which was invented out of whole cloth, just out of the richness of their imagination by Wess and Zumino turns out to be unique. It is the only mathematically allowed way of having a symmetry that connects particles of different spin.\par
And therefore without too much arbitrariness we can fasten our attention on a particular kind of symmetry which is simply called supersymmetry and explore the consequences of it. And we know that whether it's right or wrong, there isn't anything else that could unify particles of different spin. Now, we don't see any trace in nature of supermultiplets of this sort. That is, the electron does not seem to have a partner of spin-0 or spin-1. Well, that in itself should not convince us the idea is wrong. We're used by now to the idea that symmetries can be true at a fundamental level and yet spontaneously broken. Supersymmetry must be spontaneously broken. In fact, there is not a trace in nature of any supermultiplet that is visible to us. Sometimes it's said that supersymmetry is the symmetry that unifies every known particle with an unknown particle.\par
Supersymmetry is surely spontaneously broken. The big question is: where is it broken? And I've been giving a lot of attention to this question lately and I think I will close by just summarizing my conclusions. You might think perhaps that supersymmetry is broken at the same sort of energies at which the electroweak gauge symmetry is broken. That is, energies like $m_W$ or of the order of 100 GeV. There are many reasons why that doesn't work. The partners of the quarks, which are scalar particles, spin-0 particles and hence are called “squarks”, would give very fast proton decay. This could be avoided by inventing new symmetries. The “squarks” and the “sleptons” would be too light, that is, they would be light enough to have been observed and as I already told you, they haven't been observed.\par
\subsection*{Incomplete Part}
This also can be avoided by inventing new symmetries. In particular, $\phi-A$ has explored this possibility. These unwanted new symmetries and other new symmetries that you can't avoid in such theories lead to light spin-0 particles called Goldstone bosons which are apparently impossible to get rid of. And Glennys Farrar and I \cite{farrar1983supersymmetry} have been exploring their properties and we have come to the conclusion, or I should say I have come to the conclusion, I'm not sure Glennys agrees, that supersymmetry broken at these low energies is really hopeless. Another possibility is that supersymmetry is broken at medium high energy. That is, supersymmetry is broken at energies which are much larger than the $W$ mass, much smaller than the mass scale at which the Grand Unified Symmetry is broken. Now, supersymmetry...
%Acknowledgements
%Reference
%\clearpage




\bibliography{references}
\bibliographystyle{hyperamsalpha}




%\bibliography{... /ml}
\end{document}
